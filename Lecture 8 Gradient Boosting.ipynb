{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1wg22Qp3-pC2",
        "outputId": "9eb82863-f313-470a-d9b1-42059b0986f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 0.00\n",
            "R² Score: 99.99999992944922\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "X = np.array([[1, 2],\n",
        "              [2, 3],\n",
        "              [3, 4],\n",
        "              [4, 5],\n",
        "              [5, 6]])\n",
        "y = np.array([1, 1, 0, 0, 0])\n",
        "\n",
        "regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "\n",
        "regressor.fit(X, y)\n",
        "\n",
        "y_pred = regressor.predict(X)\n",
        "\n",
        "mse = np.mean((y - y_pred) ** 2)\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "\n",
        "def r2_score_percentage(y_true, y_pred):\n",
        "      # Calculate the total sum of squares (TSS)\n",
        "      tss = np.sum((y_true - np.mean(y_true)) ** 2)\n",
        "\n",
        "      # Calculate the residual sum of squares (RSS)\n",
        "      rss = np.sum((y_true - y_pred) ** 2)\n",
        "\n",
        "      # Compute the R² score\n",
        "      r2_score = 1 - (rss / tss)\n",
        "\n",
        "      # Convert R² score to percentage\n",
        "      r2_percentage = r2_score * 100\n",
        "\n",
        "      return r2_percentage\n",
        "\n",
        "r2_score = r2_score_percentage(y, y_pred)\n",
        "print(\"R² Score:\", r2_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DecisionTreeRegressor Pseudocode\n",
        "\n",
        "## Class: `DecisionTreeRegressor`\n",
        "\n",
        "### Attributes:\n",
        "- `root`: Stores the root node of the decision tree.\n",
        "\n",
        "---\n",
        "\n",
        "## Methods:\n",
        "\n",
        "### `fit(X, y)`\n",
        "**Purpose:** Build the decision tree based on the provided features `X` and target labels `y`.\n",
        "\n",
        "1. Call `_build_tree(X, y)`.\n",
        "2. Set `root` to the returned tree.\n",
        "\n",
        "---\n",
        "\n",
        "### `_build_tree(X, y)`\n",
        "**Purpose:** Recursively build the decision tree.\n",
        "\n",
        "1. Get the number of samples (`n_samples`) and features (`n_features`) in `X`.\n",
        "2. Calculate the number of unique labels in `y`.\n",
        "3. **Base Case:**\n",
        "   - If `n_labels` (unique labels in `y`) is 1:\n",
        "     - Compute the mean of labels as the leaf value using `_mean_of_labels(y)`.\n",
        "     - Return a new `Node` with this value.\n",
        "4. Find the best feature and threshold to split:\n",
        "   - Call `_best_split(X, y)`.\n",
        "   - Store `best_feature` and `best_threshold`.\n",
        "5. Split the data based on the best feature and threshold:\n",
        "   - Call `_split(X[:, best_feature], best_threshold)`.\n",
        "   - Store indices for left (`left_idxs`) and right (`right_idxs`) splits.\n",
        "6. Recursively build the left and right subtrees:\n",
        "   - Call `_build_tree` on the left split: `(X[left_idxs, :], y[left_idxs])`.\n",
        "   - Call `_build_tree` on the right split: `(X[right_idxs, :], y[right_idxs])`.\n",
        "7. Return a new `Node` with the following:\n",
        "   - `feature`: `best_feature`\n",
        "   - `threshold`: `best_threshold`\n",
        "   - `left`: Left subtree\n",
        "   - `right`: Right subtree\n",
        "\n",
        "---\n",
        "\n",
        "### `_mean_of_labels(y)`\n",
        "**Purpose:** Calculate the mean of the target labels.\n",
        "\n",
        "1. Return the mean of `y`.\n",
        "\n",
        "---\n",
        "\n",
        "### `_split(X_column, split_threshold)`\n",
        "**Purpose:** Split the data into left and right subsets based on a threshold.\n",
        "\n",
        "1. Find indices where `X_column <= split_threshold`:\n",
        "   - Store in `left_idxs`.\n",
        "2. Find indices where `X_column > split_threshold`:\n",
        "   - Store in `right_idxs`.\n",
        "3. Return `left_idxs` and `right_idxs`.\n",
        "\n",
        "---\n",
        "\n",
        "### `_best_split(X, y)`\n",
        "**Purpose:** Identify the best feature and threshold to split the data.\n",
        "\n",
        "1. Initialize variables:\n",
        "   - `best_gain = infinity`\n",
        "   - `split_idx = None`\n",
        "   - `split_threshold = None`\n",
        "2. For each feature in `X`:\n",
        "   - Extract the feature column.\n",
        "   - Sort the values.\n",
        "   - Compute midpoints of consecutive values as potential thresholds.\n",
        "3. For each threshold:\n",
        "   - Compute the information gain using `_information_gain(y, X_column, threshold)`.\n",
        "   - If the gain is better than `best_gain`:\n",
        "     - Update `best_gain`, `split_idx`, and `split_threshold`.\n",
        "4. Return `split_idx` and `split_threshold`.\n",
        "\n",
        "---\n",
        "\n",
        "### `_information_gain(y, X_column, threshold)`\n",
        "**Purpose:** Compute the information gain of a potential split.\n",
        "\n",
        "1. Split the data using `_split(X_column, threshold)`:\n",
        "   - Get `left_idxs` and `right_idxs`.\n",
        "2. **Edge Case:** If either split is empty, return `0`.\n",
        "3. Compute:\n",
        "   - Total number of samples: `n`.\n",
        "   - Number of left and right samples: `n_l`, `n_r`.\n",
        "   - Entropy of left and right splits: `e_l`, `e_r`.\n",
        "4. Calculate weighted entropy of the split:\n",
        "   - `(n_l / n) * e_l + (n_r / n) * e_r`.\n",
        "5. Return the information gain.\n",
        "\n",
        "---\n",
        "\n",
        "### `_entropy(y)`\n",
        "**Purpose:** Compute the variance (proxy for entropy) of labels.\n",
        "\n",
        "1. Compute the mean squared deviation of `y`.\n",
        "2. Return the variance.\n",
        "\n",
        "---\n",
        "\n",
        "### `predict(X)`\n",
        "**Purpose:** Predict the output for input data `X`.\n",
        "\n",
        "1. For each sample in `X`:\n",
        "   - Traverse the tree starting from the root using `_traverse_tree`.\n",
        "   - Store the prediction.\n",
        "2. Return all predictions as an array.\n",
        "\n",
        "---\n",
        "\n",
        "### `_traverse_tree(x, node)`\n",
        "**Purpose:** Traverse the decision tree to make a prediction for a single sample.\n",
        "\n",
        "1. If `node` is a leaf node:\n",
        "   - Return `node.value`.\n",
        "2. Else:\n",
        "   - If `x[node.feature] <= node.threshold`:\n",
        "     - Recursively traverse the left subtree.\n",
        "   - Otherwise, traverse the right subtree.\n",
        "3. Return the result from the recursive traversal.\n"
      ],
      "metadata": {
        "id": "endTqUwdvhCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Node:\n",
        "    def __init__(self, feature=None, threshold=None, left=None, right=None, *, value=None):\n",
        "        self.feature = feature\n",
        "        self.threshold = threshold\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "        self.value = value\n",
        "\n",
        "    def is_leaf_node(self):\n",
        "        return self.value is not None\n",
        "\n",
        "\n",
        "class DecisionTreeRegressor:\n",
        "    def __init__(self):\n",
        "        self.root = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.root = self._build_tree(X, y)\n",
        "\n",
        "    def _build_tree(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        n_labels = len(np.unique(y))\n",
        "\n",
        "        if(n_labels == 1):\n",
        "          leaf_value = self._mean_of_labels(y)\n",
        "          return Node(value=leaf_value)\n",
        "\n",
        "        best_feature, best_threshold = self._best_split(X, y)\n",
        "\n",
        "        left_idxs, right_idxs = self._split(X[:, best_feature], best_threshold)\n",
        "        left = self._build_tree(X[left_idxs, :], y[left_idxs])\n",
        "        right = self._build_tree(X[right_idxs, :], y[right_idxs])\n",
        "        return Node(best_feature, best_threshold, left, right)\n",
        "\n",
        "    def _mean_of_labels(self, y):\n",
        "        return np.mean(y)\n",
        "\n",
        "    def _split(self, X_column, split_threshold):\n",
        "        left_idxs = np.argwhere(X_column <= split_threshold).flatten()\n",
        "        right_idxs = np.argwhere(X_column > split_threshold).flatten()\n",
        "        return left_idxs, right_idxs\n",
        "\n",
        "    def _best_split(self, X, y):\n",
        "        best_gain, split_idx, split_threshold = float(\"inf\"), None, None\n",
        "\n",
        "        for feat_idx in range(X.shape[1]):\n",
        "            X_Column = X[:, feat_idx]\n",
        "            X_Column_sorted = np.sort(X_Column)\n",
        "            thresholds = (X_Column_sorted[:-1] + X_Column_sorted[1:])/2\n",
        "\n",
        "            for threshold in thresholds:\n",
        "                gain = self._information_gain(y, X_Column, threshold)\n",
        "                if gain < best_gain:\n",
        "                    best_gain = gain\n",
        "                    split_idx = feat_idx\n",
        "                    split_threshold = threshold\n",
        "\n",
        "        return split_idx, split_threshold\n",
        "\n",
        "    def _information_gain(self, y, X_column, threshold):\n",
        "      # parent_entropy = self._entropy(y)\n",
        "\n",
        "      left_idxs, right_idxs = self._split(X_column, threshold)\n",
        "\n",
        "      if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
        "        return 0\n",
        "\n",
        "      n, n_l, n_r = len(y), len(left_idxs), len(right_idxs)\n",
        "      e_l, e_r = self._entropy(y[left_idxs]), self._entropy(y[right_idxs])\n",
        "      information_gain = (n_l/n)*e_l + (n_r/n)*e_r\n",
        "      return information_gain\n",
        "\n",
        "    def _entropy(self, y):\n",
        "      return np.mean((y - np.mean(y)) ** 2)\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = np.array([self._traverse_tree(x, self.root) for x in X])\n",
        "        return predictions\n",
        "\n",
        "    def _traverse_tree(self, x, node):\n",
        "        if node.is_leaf_node():\n",
        "            return node.value\n",
        "\n",
        "        if x[node.feature] <= node.threshold:\n",
        "            return self._traverse_tree(x, node.left)\n",
        "        return self._traverse_tree(x, node.right)\n",
        "\n",
        "X = np.array([[1, 2],\n",
        "              [2, 3],\n",
        "              [3, 4],\n",
        "              [4, 5],\n",
        "              [5, 6]])\n",
        "y = np.array([1, 1, 0, 0, 0])\n",
        "\n",
        "regressor = DecisionTreeRegressor()\n",
        "regressor.fit(X, y)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = regressor.predict(X)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = np.mean((y - y_pred) ** 2)\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "\n",
        "def r2_score_percentage(y_true, y_pred):\n",
        "      # Calculate the total sum of squares (TSS)\n",
        "      tss = np.sum((y_true - np.mean(y_true)) ** 2)\n",
        "\n",
        "      # Calculate the residual sum of squares (RSS)\n",
        "      rss = np.sum((y_true - y_pred) ** 2)\n",
        "\n",
        "      # Compute the R² score\n",
        "      r2_score = 1 - (rss / tss)\n",
        "\n",
        "      # Convert R² score to percentage\n",
        "      r2_percentage = r2_score * 100\n",
        "\n",
        "      return r2_percentage\n",
        "\n",
        "r2_score = r2_score_percentage(y, y_pred)\n",
        "print(\"R² Score:\", r2_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvmcSHKKmEQb",
        "outputId": "cba9c0e5-c748-45d5-fb6a-18055ce05f78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 0.00\n",
            "R² Score: 100.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DecisionTreeRegressor Pseudocode\n",
        "\n",
        "## Class: `DecisionTreeRegressor`\n",
        "\n",
        "### Attributes:\n",
        "- `root`: Stores the root node of the decision tree.\n",
        "\n",
        "---\n",
        "\n",
        "## Methods:\n",
        "\n",
        "### `fit(X, y)`\n",
        "**Purpose:** Build the decision tree based on the provided features `X` and target labels `y`.\n",
        "\n",
        "1. Call `_build_tree(X, y)`.\n",
        "2. Set `root` to the returned tree.\n",
        "\n",
        "---\n",
        "\n",
        "### `_build_tree(X, y)`\n",
        "**Purpose:** Recursively build the decision tree.\n",
        "\n",
        "1. Determine the number of samples (`n_samples`) and features (`n_features`) in `X`.\n",
        "2. Calculate the number of unique labels in `y` as `n_labels`.\n",
        "3. **Base Case:**\n",
        "   - If `n_labels` is 1 (all labels are identical):\n",
        "     - Compute the mean of labels as the leaf value using `_mean_of_labels(y)`.\n",
        "     - Return a new `Node` with `value=leaf_value`.\n",
        "4. Find the best feature and threshold to split:\n",
        "   - Call `_best_split(X, y)`.\n",
        "   - Store `best_feature` and `best_threshold`.\n",
        "5. Split the data into left and right subsets:\n",
        "   - Call `_split(X[:, best_feature], best_threshold)`.\n",
        "   - Store indices for left (`left_idxs`) and right (`right_idxs`) splits.\n",
        "6. Recursively build the left and right subtrees:\n",
        "   - Call `_build_tree(X[left_idxs, :], y[left_idxs])` for the left subtree.\n",
        "   - Call `_build_tree(X[right_idxs, :], y[right_idxs])` for the right subtree.\n",
        "7. Return a new `Node` with:\n",
        "   - `feature`: `best_feature`\n",
        "   - `threshold`: `best_threshold`\n",
        "   - `left`: Left subtree\n",
        "   - `right`: Right subtree\n",
        "\n",
        "---\n",
        "\n",
        "### `_mean_of_labels(y)`\n",
        "**Purpose:** Calculate the mean of the target labels.\n",
        "\n",
        "1. Return the mean of `y`.\n",
        "\n",
        "---\n",
        "\n",
        "### `_split(X_column, split_threshold)`\n",
        "**Purpose:** Split the data into left and right subsets based on a threshold.\n",
        "\n",
        "1. Identify indices where `X_column <= split_threshold`:\n",
        "   - Store in `left_idxs`.\n",
        "2. Identify indices where `X_column > split_threshold`:\n",
        "   - Store in `right_idxs`.\n",
        "3. Return `left_idxs` and `right_idxs`.\n",
        "\n",
        "---\n",
        "\n",
        "### `_best_split(X, y)`\n",
        "**Purpose:** Identify the best feature and threshold to split the data.\n",
        "\n",
        "1. Initialize variables:\n",
        "   - `best_gain = infinity`\n",
        "   - `split_idx = None`\n",
        "   - `split_threshold = None`\n",
        "2. Iterate over each feature in `X`:\n",
        "   - Extract the feature column.\n",
        "   - Sort the feature values.\n",
        "   - Compute midpoints between consecutive values as potential thresholds.\n",
        "3. For each threshold:\n",
        "   - Compute the information gain using `_information_gain(y, X_column, threshold)`.\n",
        "   - If the gain is better (lower) than `best_gain`:\n",
        "     - Update `best_gain`, `split_idx`, and `split_threshold`.\n",
        "4. Return `split_idx` and `split_threshold`.\n",
        "\n",
        "---\n",
        "\n",
        "### `_information_gain(y, X_column, threshold)`\n",
        "**Purpose:** Compute the information gain for a potential split.\n",
        "\n",
        "1. Split the data using `_split(X_column, threshold)`:\n",
        "   - Obtain `left_idxs` and `right_idxs`.\n",
        "2. **Edge Case:** If either split is empty, return `0`.\n",
        "3. Compute:\n",
        "   - Total samples: `n`.\n",
        "   - Number of samples in left and right splits: `n_l`, `n_r`.\n",
        "   - Variance (entropy proxy) of left and right splits: `e_l`, `e_r`.\n",
        "4. Calculate the weighted variance:\n",
        "   - `(n_l / n) * e_l + (n_r / n) * e_r`.\n",
        "5. Return the information gain.\n",
        "\n",
        "---\n",
        "\n",
        "### `_entropy(y)`\n",
        "**Purpose:** Compute the variance (proxy for entropy) of the labels.\n",
        "\n",
        "1. Compute the mean squared deviation of `y`:\n",
        "   - `(y - mean(y))^2`.\n",
        "2. Return the variance.\n",
        "\n",
        "---\n",
        "\n",
        "### `predict(X)`\n",
        "**Purpose:** Predict the output for input data `X`.\n",
        "\n",
        "1. For each sample in `X`:\n",
        "   - Traverse the tree starting from the root using `_traverse_tree`.\n",
        "   - Store the prediction.\n",
        "2. Return all predictions as an array.\n",
        "\n",
        "---\n",
        "\n",
        "### `_traverse_tree(x, node)`\n",
        "**Purpose:** Traverse the decision tree to make a prediction for a single sample.\n",
        "\n",
        "1. If `node` is a leaf node:\n",
        "   - Return `node.value`.\n",
        "2. Otherwise:\n",
        "   - If `x[node.feature] <= node.threshold`:\n",
        "     - Recursively traverse the left subtree.\n",
        "   - Otherwise, recursively traverse the right subtree.\n",
        "3. Return the result from the recursive traversal.\n"
      ],
      "metadata": {
        "id": "iE-8TIuMwKYL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Node:\n",
        "    def __init__(self, feature=None, threshold=None, left=None, right=None, *, value=None):\n",
        "        self.feature = feature\n",
        "        self.threshold = threshold\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "        self.value = value\n",
        "\n",
        "    def is_leaf_node(self):\n",
        "        return self.value is not None\n",
        "\n",
        "\n",
        "class DecisionTreeRegressor:\n",
        "    def __init__(self, max_depth=3):\n",
        "        self.max_depth = max_depth\n",
        "        self.root = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.root = self._build_tree(X, y)\n",
        "\n",
        "    def _build_tree(self, X, y, depth=0):\n",
        "        n_samples, n_features = X.shape\n",
        "        n_labels = len(np.unique(y))\n",
        "\n",
        "        if(n_labels == 1 or self.max_depth is not None and depth >= self.max_depth):\n",
        "          leaf_value = self._mean_of_labels(y)\n",
        "          return Node(value=leaf_value)\n",
        "\n",
        "        best_feature, best_threshold = self._best_split(X, y)\n",
        "\n",
        "        left_idxs, right_idxs = self._split(X[:, best_feature], best_threshold)\n",
        "        left = self._build_tree(X[left_idxs, :], y[left_idxs], depth+1)\n",
        "        right = self._build_tree(X[right_idxs, :], y[right_idxs], depth+1)\n",
        "        return Node(best_feature, best_threshold, left, right)\n",
        "\n",
        "    def _mean_of_labels(self, y):\n",
        "        return np.mean(y)\n",
        "\n",
        "    def _split(self, X_column, split_threshold):\n",
        "        left_idxs = np.argwhere(X_column <= split_threshold).flatten()\n",
        "        right_idxs = np.argwhere(X_column > split_threshold).flatten()\n",
        "        return left_idxs, right_idxs\n",
        "\n",
        "    def _best_split(self, X, y):\n",
        "        best_gain, split_idx, split_threshold = float(\"inf\"), None, None\n",
        "\n",
        "        for feat_idx in range(X.shape[1]):\n",
        "            X_Column = X[:, feat_idx]\n",
        "            X_Column_sorted = np.sort(X_Column)\n",
        "            thresholds = (X_Column_sorted[:-1] + X_Column_sorted[1:])/2\n",
        "\n",
        "            for threshold in thresholds:\n",
        "                gain = self._information_gain(y, X_Column, threshold)\n",
        "                if gain < best_gain:\n",
        "                    best_gain = gain\n",
        "                    split_idx = feat_idx\n",
        "                    split_threshold = threshold\n",
        "\n",
        "        return split_idx, split_threshold\n",
        "\n",
        "    def _information_gain(self, y, X_column, threshold):\n",
        "      # parent_entropy = self._entropy(y)\n",
        "\n",
        "      left_idxs, right_idxs = self._split(X_column, threshold)\n",
        "\n",
        "      if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
        "        return 0\n",
        "\n",
        "      n, n_l, n_r = len(y), len(left_idxs), len(right_idxs)\n",
        "      e_l, e_r = self._entropy(y[left_idxs]), self._entropy(y[right_idxs])\n",
        "      information_gain = (n_l/n)*e_l + (n_r/n)*e_r\n",
        "      return information_gain\n",
        "\n",
        "    def _entropy(self, y):\n",
        "      return np.mean((y - np.mean(y)) ** 2)\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = np.array([self._traverse_tree(x, self.root) for x in X])\n",
        "        return predictions\n",
        "\n",
        "    def _traverse_tree(self, x, node):\n",
        "        if node.is_leaf_node():\n",
        "            return node.value\n",
        "\n",
        "        if x[node.feature] <= node.threshold:\n",
        "            return self._traverse_tree(x, node.left)\n",
        "        return self._traverse_tree(x, node.right)\n",
        "\n",
        "X = np.array([[1, 2],\n",
        "              [2, 3],\n",
        "              [3, 4],\n",
        "              [4, 5],\n",
        "              [5, 6]])\n",
        "y = np.array([1, 1, 0, 0, 0])  # Labels must be -1 or 1\n",
        "\n",
        "regressor = DecisionTreeRegressor(max_depth=3)\n",
        "regressor.fit(X, y)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = regressor.predict(X)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = np.mean((y - y_pred) ** 2)\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "\n",
        "def r2_score_percentage(y_true, y_pred):\n",
        "      # Calculate the total sum of squares (TSS)\n",
        "      tss = np.sum((y_true - np.mean(y_true)) ** 2)\n",
        "\n",
        "      # Calculate the residual sum of squares (RSS)\n",
        "      rss = np.sum((y_true - y_pred) ** 2)\n",
        "\n",
        "      # Compute the R² score\n",
        "      r2_score = 1 - (rss / tss)\n",
        "\n",
        "      # Convert R² score to percentage\n",
        "      r2_percentage = r2_score * 100\n",
        "\n",
        "      return r2_percentage\n",
        "\n",
        "r2_score = r2_score_percentage(y, y_pred)\n",
        "print(\"R² Score:\", r2_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQJE0EamniKj",
        "outputId": "d7cfba25-f888-4d2a-ca8b-3d8d32a60619"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 0.00\n",
            "R² Score: 100.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GradientBoostingRegressor Pseudocode\n",
        "\n",
        "## Class: `GradientBoostingRegressor`\n",
        "\n",
        "### Attributes:\n",
        "- `n_estimators`: Number of trees (iterations) to build.\n",
        "- `learning_rate`: Shrinkage factor applied to the predictions of each tree.\n",
        "- `max_depth`: Maximum depth of each decision tree.\n",
        "- `trees`: List to store all decision trees.\n",
        "\n",
        "---\n",
        "\n",
        "## Methods:\n",
        "\n",
        "### `__init__(n_estimators=100, learning_rate=0.1, max_depth=1)`\n",
        "**Purpose:** Initialize the regressor with the given parameters.\n",
        "\n",
        "1. Set `self.n_estimators` to `n_estimators`.\n",
        "2. Set `self.learning_rate` to `learning_rate`.\n",
        "3. Set `self.max_depth` to `max_depth`.\n",
        "4. Initialize `self.trees` as an empty list.\n",
        "\n",
        "---\n",
        "\n",
        "### `fit(X, y)`\n",
        "**Purpose:** Train the gradient boosting model on the input data `X` and targets `y`.\n",
        "\n",
        "1. Calculate the number of samples `m = len(y)`.\n",
        "2. Initialize the model's baseline prediction as the mean of the target values:\n",
        "   - `self.initial_prediction = mean(y)`.\n",
        "3. Compute the initial residuals as:\n",
        "   - `residuals = y - self.initial_prediction`.\n",
        "4. **Iterate** for `n_estimators` steps:\n",
        "   - Create a new `DecisionTreeRegressor` with `max_depth=self.max_depth`.\n",
        "   - Fit the tree on the residuals:\n",
        "     - `tree.fit(X, residuals)`.\n",
        "   - Predict the residuals using the current tree:\n",
        "     - `predictions = tree.predict(X)`.\n",
        "   - Update the residuals by subtracting the learning rate-scaled predictions:\n",
        "     - `residuals -= self.learning_rate * predictions`.\n",
        "   - Add the tree to the `self.trees` list.\n",
        "\n",
        "---\n",
        "\n",
        "### `predict(X)`\n",
        "**Purpose:** Predict target values for the input data `X`.\n",
        "\n",
        "1. Initialize predictions with the baseline prediction:\n",
        "   - `y_pred = array of size X.shape[0], filled with self.initial_prediction`.\n",
        "2. **For each tree** in `self.trees`:\n",
        "   - Add the learning rate-scaled predictions of the tree to `y_pred`:\n",
        "     - `y_pred += self.learning_rate * tree.predict(X)`.\n",
        "3. Return `y_pred` as the final predictions.\n"
      ],
      "metadata": {
        "id": "JwXW_omQwatD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Node:\n",
        "    def __init__(self, feature=None, threshold=None, left=None, right=None, *, value=None):\n",
        "        self.feature = feature\n",
        "        self.threshold = threshold\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "        self.value = value\n",
        "\n",
        "    def is_leaf_node(self):\n",
        "        return self.value is not None\n",
        "\n",
        "\n",
        "class DecisionTreeRegressor:\n",
        "    def __init__(self, max_depth=3):\n",
        "        self.max_depth = max_depth\n",
        "        self.root = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.root = self._build_tree(X, y)\n",
        "\n",
        "    def _build_tree(self, X, y, depth=0):\n",
        "        n_samples, n_features = X.shape\n",
        "        n_labels = len(np.unique(y))\n",
        "\n",
        "        if(n_labels == 1 or self.max_depth is not None and depth >= self.max_depth):\n",
        "          leaf_value = self._mean_of_labels(y)\n",
        "          return Node(value=leaf_value)\n",
        "        # if(n_labels == 1):\n",
        "        #   leaf_value = self._mean_of_labels(y)\n",
        "        #   return Node(value=leaf_value)\n",
        "\n",
        "        best_feature, best_threshold = self._best_split(X, y)\n",
        "\n",
        "        left_idxs, right_idxs = self._split(X[:, best_feature], best_threshold)\n",
        "        left = self._build_tree(X[left_idxs, :], y[left_idxs], depth + 1)\n",
        "        right = self._build_tree(X[right_idxs, :], y[right_idxs], depth + 1)\n",
        "        return Node(best_feature, best_threshold, left, right)\n",
        "\n",
        "    def _mean_of_labels(self, y):\n",
        "        return np.mean(y)\n",
        "\n",
        "    def _split(self, X_column, split_threshold):\n",
        "        left_idxs = np.argwhere(X_column <= split_threshold).flatten()\n",
        "        right_idxs = np.argwhere(X_column > split_threshold).flatten()\n",
        "        return left_idxs, right_idxs\n",
        "\n",
        "    def _best_split(self, X, y):\n",
        "        best_gain, split_idx, split_threshold = float(\"inf\"), None, None\n",
        "\n",
        "        for feat_idx in range(X.shape[1]):\n",
        "            X_Column = X[:, feat_idx]\n",
        "            X_Column_sorted = np.sort(X_Column)\n",
        "            thresholds = (X_Column_sorted[:-1] + X_Column_sorted[1:])/2\n",
        "\n",
        "            for threshold in thresholds:\n",
        "                gain = self._information_gain(y, X_Column, threshold)\n",
        "                if gain < best_gain:\n",
        "                    best_gain = gain\n",
        "                    split_idx = feat_idx\n",
        "                    split_threshold = threshold\n",
        "\n",
        "        return split_idx, split_threshold\n",
        "\n",
        "    def _information_gain(self, y, X_column, threshold):\n",
        "      # parent_entropy = self._entropy(y)\n",
        "\n",
        "      left_idxs, right_idxs = self._split(X_column, threshold)\n",
        "\n",
        "      if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
        "        return 0\n",
        "\n",
        "      n, n_l, n_r = len(y), len(left_idxs), len(right_idxs)\n",
        "      e_l, e_r = self._entropy(y[left_idxs]), self._entropy(y[right_idxs])\n",
        "      information_gain = (n_l/n)*e_l + (n_r/n)*e_r\n",
        "      return information_gain\n",
        "\n",
        "    def _entropy(self, y):\n",
        "      return np.mean((y - np.mean(y)) ** 2)\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = np.array([self._traverse_tree(x, self.root) for x in X])\n",
        "        return predictions\n",
        "\n",
        "    def _traverse_tree(self, x, node):\n",
        "        if node.is_leaf_node():\n",
        "            return node.value\n",
        "\n",
        "        if x[node.feature] <= node.threshold:\n",
        "            return self._traverse_tree(x, node.left)\n",
        "        return self._traverse_tree(x, node.right)\n",
        "\n",
        "\n",
        "class GradientBoostingRegressor:\n",
        "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=1):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_depth = max_depth\n",
        "        self.trees = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        m = len(y)\n",
        "        self.initial_prediction = np.mean(y)\n",
        "        residuals = y - self.initial_prediction\n",
        "\n",
        "        for _ in range(self.n_estimators):\n",
        "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
        "            tree.fit(X, residuals)\n",
        "            predictions = tree.predict(X)\n",
        "            residuals -= self.learning_rate * predictions\n",
        "            self.trees.append(tree)\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_pred = np.full(X.shape[0], self.initial_prediction)\n",
        "        for tree in self.trees:\n",
        "            y_pred += self.learning_rate * tree.predict(X)\n",
        "        return y_pred\n",
        "X = np.array([[1, 2],\n",
        "              [2, 3],\n",
        "              [3, 4],\n",
        "              [4, 5],\n",
        "              [5, 6]])\n",
        "y = np.array([1, 1, 0, 0, 0])\n",
        "\n",
        "regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=10)\n",
        "regressor.fit(X, y)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = regressor.predict(X)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = np.mean((y - y_pred) ** 2)\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "\n",
        "def r2_score_percentage(y_true, y_pred):\n",
        "      # Calculate the total sum of squares (TSS)\n",
        "      tss = np.sum((y_true - np.mean(y_true)) ** 2)\n",
        "\n",
        "      # Calculate the residual sum of squares (RSS)\n",
        "      rss = np.sum((y_true - y_pred) ** 2)\n",
        "\n",
        "      # Compute the R² score\n",
        "      r2_score = 1 - (rss / tss)\n",
        "\n",
        "      # Convert R² score to percentage\n",
        "      r2_percentage = r2_score * 100\n",
        "\n",
        "      return r2_percentage\n",
        "\n",
        "r2_score = r2_score_percentage(y, y_pred)\n",
        "print(\"R² Score:\", r2_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OoXekmKZp8Dj",
        "outputId": "293411aa-99a2-453d-ecb7-e6ddba3f0e81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 0.00\n",
            "R² Score: 99.99999992944922\n"
          ]
        }
      ]
    }
  ]
}