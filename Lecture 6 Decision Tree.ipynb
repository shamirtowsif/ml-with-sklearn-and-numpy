{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Initialize the Tree\n",
        "   - Define a `Node` class to represent each node of the tree.\n",
        "   - Define the `DecisionTree` class, with a `fit` function that initiates the process of building the tree using training data `X` and target labels `y`."
      ],
      "metadata": {
        "id": "fWgZC9nikD6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Node:\n",
        "  def __init__(self, feature=None, threshold=None, left=None, right=None, *, value=None):\n",
        "    self.feature = feature\n",
        "    self.threshold = threshold\n",
        "    self.left = left\n",
        "    self.right = right\n",
        "    self.value = value\n",
        "\n",
        "  def is_leaf_node(self):\n",
        "    return self.value is not None\n",
        "\n",
        "\n",
        "class DecisionTree:\n",
        "  def __init__(self):\n",
        "    self.root = None\n",
        "\n",
        "  def fit(self, X, y):\n",
        "    pass"
      ],
      "metadata": {
        "id": "FuXMmeGskPcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Grow the Tree Recursively (`_grow_tree` function)\n",
        "   - For the current subset of `X` and `y`:\n",
        "     - **Check Stopping Criteria**:\n",
        "       - If all samples belong to one class, create a leaf node with this class as its value.\n",
        "     - **Select Features**:\n",
        "       - Select feature indices to consider for the split (in this implementation, all features are used).\n",
        "     - **Find Best Split** (`_best_split` function):\n",
        "       - For each feature:\n",
        "         - Sort values of that feature.\n",
        "         - Calculate potential split thresholds by finding midpoints between consecutive sorted values.\n",
        "         - For each threshold, calculate the information gain and track the best one.\n",
        "     - **Split Data**:\n",
        "       - Split `X` and `y` into two subsets based on the best feature and threshold.\n",
        "     - **Recursive Calls**:\n",
        "       - Recursively grow left and right subtrees with the split data subsets.\n",
        "     - **Return Node**:\n",
        "       - Return a node with the selected feature and threshold, and attach the left and right subtrees.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iywSgdZIlw7i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class DecisionTree:\n",
        "  def __init__(self):\n",
        "    self.root = None\n",
        "\n",
        "  def fit(self, X, y):\n",
        "    self.root = self._build_tree(X, y)\n",
        "\n",
        "  def _build_tree(self, X, y):\n",
        "    n_samples, n_features = X.shape\n",
        "    n_labels = len(np.unique(y))\n",
        "\n",
        "    if(n_labels == 1):\n",
        "      leaf_value = self._most_common_label(y)\n",
        "      return Node(value=leaf_value)\n",
        "\n",
        "    feat_idx = np.arange(n_features)\n",
        "\n",
        "    best_feature, best_threshold = self._best_split(X, y, feat_idx)\n",
        "\n",
        "    left_idxs , right_idxs = self._split(X[:, best_feature], best_threshold)\n",
        "    left = self._build_tree(X[left_idxs, :], y[left_idxs])\n",
        "    right = self._build_tree(X[right_idxs, :], y[right_idxs])\n",
        "    return Node(best_feature, best_threshold, left, right)\n",
        "\n",
        "  def _most_common_label(self, y):\n",
        "    unique_labels, counts = np.unique(y, return_counts=True)\n",
        "    return unique_labels[np.argmax(counts)]\n",
        "\n",
        "  def _split(self, X_column, split_threshold):\n",
        "    left_idxs = np.argwhere(X_column <= split_threshold).flatten()\n",
        "    right_idxs = np.argwhere(X_column > split_threshold).flatten()\n",
        "    return left_idxs, right_idxs\n",
        "\n",
        "  def _best_split(self, X, y, feat_idxs):\n",
        "    best_gain, split_idx, split_threshold = -1, None, None\n",
        "\n",
        "    for feat_idx in feat_idxs:\n",
        "      X_Column = X[:, feat_idx]\n",
        "      X_Column_sorted = np.sort(X_Column)\n",
        "      thresholds = (X_Column_sorted[:-1] + X_Column_sorted[1:])/2\n",
        "\n",
        "      for threshold in thresholds:\n",
        "        gain = self._information_gain(y, X_Column, threshold)\n",
        "\n",
        "        if gain > best_gain:\n",
        "          best_gain = gain\n",
        "          split_idx = feat_idx\n",
        "          split_threshold = threshold\n",
        "\n",
        "    return split_idx, split_threshold\n"
      ],
      "metadata": {
        "id": "h89Y0uH4nvCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Evaluate Split Quality (`_information_gain` function)\n",
        "   - **Split** (`_split` function):\n",
        "     - Divide samples into left and right subsets based on the split threshold.\n",
        "   - **Child Entropy**:\n",
        "     - Compute weighted average of left and right child entropies based on their sizes."
      ],
      "metadata": {
        "id": "c05GyYZqsRWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class DecisionTree:\n",
        "  def __init__(self):\n",
        "    self.root = None\n",
        "\n",
        "  def fit(self, X, y):\n",
        "    self.root = self._build_tree(X, y)\n",
        "\n",
        "  def _build_tree(self, X, y):\n",
        "    n_samples, n_features = X.shape\n",
        "    n_labels = len(np.unique(y))\n",
        "\n",
        "    if(n_labels == 1):\n",
        "      leaf_value = self._most_common_label(y)\n",
        "      return Node(value=leaf_value)\n",
        "\n",
        "    feat_idx = np.arange(n_features)\n",
        "\n",
        "    best_feature, best_threshold = self._best_split(X, y, feat_idx)\n",
        "\n",
        "    left_idxs , right_idxs = self._split(X[:, best_feature], best_threshold)\n",
        "    left = self._build_tree(X[left_idxs, :], y[left_idxs])\n",
        "    right = self._build_tree(X[right_idxs, :], y[right_idxs])\n",
        "    return Node(best_feature, best_threshold, left, right)\n",
        "\n",
        "  def _most_common_label(self, y):\n",
        "    unique_labels, counts = np.unique(y, return_counts=True)\n",
        "    return unique_labels[np.argmax(counts)]\n",
        "\n",
        "  def _split(self, X_column, split_threshold):\n",
        "    left_idxs = np.argwhere(X_column <= split_threshold).flatten()\n",
        "    right_idxs = np.argwhere(X_column > split_threshold).flatten()\n",
        "    return left_idxs, right_idxs\n",
        "\n",
        "  def _best_split(self, X, y, feat_idxs):\n",
        "    best_gain, split_idx, split_threshold = float('inf'), None, None\n",
        "\n",
        "    for feat_idx in feat_idxs:\n",
        "      X_Column = X[:, feat_idx]\n",
        "      X_Column_sorted = np.sort(X_Column)\n",
        "      thresholds = (X_Column_sorted[:-1] + X_Column_sorted[1:])/2\n",
        "\n",
        "      for threshold in thresholds:\n",
        "        gain = self._information_gain(y, X_Column, threshold)\n",
        "\n",
        "        if gain < best_gain:\n",
        "          best_gain = gain\n",
        "          split_idx = feat_idx\n",
        "          split_threshold = threshold\n",
        "\n",
        "    return split_idx, split_threshold\n",
        "\n",
        "  def _information_gain(self, y, X_column, threshold):\n",
        "    # parent_entropy = self._entropy(y)\n",
        "\n",
        "    left_idxs, right_idxs = self._split(X_column, threshold)\n",
        "\n",
        "    if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
        "      return 0\n",
        "\n",
        "    n, n_l, n_r = len(y), len(left_idxs), len(right_idxs)\n",
        "    e_l, e_r = self._entropy(y[left_idxs]), self._entropy(y[right_idxs])\n",
        "    child_entropy = (n_l/n)*e_l + (n_r/n)*e_r\n",
        "    # information_gain = parent_entropy - child_entropy\n",
        "    information_gain = child_entropy\n",
        "    return information_gain\n",
        "\n",
        "  def _entropy(self, y):\n",
        "    pass"
      ],
      "metadata": {
        "id": "vQd1FNbrsfW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Entropy Calculation (`_entropy` function)\n",
        "   - Calculate the entropy for a subset `y`:\n",
        "     - Compute probabilities for each class and use the entropy formula."
      ],
      "metadata": {
        "id": "GpWK8VEn1iIl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class DecisionTree:\n",
        "  def __init__(self):\n",
        "    self.root = None\n",
        "\n",
        "  def fit(self, X, y):\n",
        "    self.root = self._build_tree(X, y)\n",
        "\n",
        "  def _build_tree(self, X, y):\n",
        "    n_samples, n_features = X.shape\n",
        "    n_labels = len(np.unique(y))\n",
        "\n",
        "    if(n_labels == 1):\n",
        "      leaf_value = self._most_common_label(y)\n",
        "      return Node(value=leaf_value)\n",
        "\n",
        "    feat_idx = np.arange(n_features)\n",
        "\n",
        "    best_feature, best_threshold = self._best_split(X, y, feat_idx)\n",
        "\n",
        "    left_idxs , right_idxs = self._split(X[:, best_feature], best_threshold)\n",
        "    left = self._build_tree(X[left_idxs, :], y[left_idxs])\n",
        "    right = self._build_tree(X[right_idxs, :], y[right_idxs])\n",
        "    return Node(best_feature, best_threshold, left, right)\n",
        "\n",
        "  def _most_common_label(self, y):\n",
        "    unique_labels, counts = np.unique(y, return_counts=True)\n",
        "    return unique_labels[np.argmax(counts)]\n",
        "\n",
        "  def _split(self, X_column, split_threshold):\n",
        "    left_idxs = np.argwhere(X_column <= split_threshold).flatten()\n",
        "    right_idxs = np.argwhere(X_column > split_threshold).flatten()\n",
        "    return left_idxs, right_idxs\n",
        "\n",
        "  def _best_split(self, X, y, feat_idxs):\n",
        "    best_gain, split_idx, split_threshold = -1, None, None\n",
        "\n",
        "    for feat_idx in feat_idxs:\n",
        "      X_Column = X[:, feat_idx]\n",
        "      X_Column_sorted = np.sort(X_Column)\n",
        "      thresholds = (X_Column_sorted[:-1] + X_Column_sorted[1:])/2\n",
        "\n",
        "      for threshold in thresholds:\n",
        "        gain = self._information_gain(y, X_Column, threshold)\n",
        "\n",
        "        if gain > best_gain:\n",
        "          best_gain = gain\n",
        "          split_idx = feat_idx\n",
        "          split_threshold = threshold\n",
        "\n",
        "    return split_idx, split_threshold\n",
        "\n",
        "  def _information_gain(self, y, X_column, threshold):\n",
        "    # parent_entropy = self._entropy(y)\n",
        "\n",
        "    left_idxs, right_idxs = self._split(X_column, threshold)\n",
        "\n",
        "    if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
        "      return 0\n",
        "\n",
        "    n, n_l, n_r = len(y), len(left_idxs), len(right_idxs)\n",
        "    e_l, e_r = self._entropy(y[left_idxs]), self._entropy(y[right_idxs])\n",
        "    child_entropy = (n_l/n)*e_l + (n_r/n)*e_r\n",
        "    # information_gain = parent_entropy - child_entropy\n",
        "    information_gain = child_entropy\n",
        "    return information_gain\n",
        "\n",
        "  def _entropy(self, y):\n",
        "    fid3 = np.mean(y)\n",
        "\n",
        "    if fid3 == 0 or fid3 == 1:\n",
        "      return 0\n",
        "    else:\n",
        "      return -fid3*np.log2(fid3) - (1-fid3)*np.log2(1-fid3)"
      ],
      "metadata": {
        "id": "HjIaHUCd1lPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Predict New Samples (`predict` function)\n",
        "   - For each sample `x` in `X`:\n",
        "     - **Traverse Tree** (`_traverse_tree` function):\n",
        "       - Start at the root and navigate down:\n",
        "         - Check if `x` satisfies the threshold condition at each node.\n",
        "         - Continue left or right until reaching a leaf node.\n",
        "       - **Return Prediction**:\n",
        "         - Output the value (class) of the leaf node reached."
      ],
      "metadata": {
        "id": "Yrz99ic81zSs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class DecisionTree:\n",
        "  def __init__(self):\n",
        "    self.root = None\n",
        "\n",
        "  def fit(self, X, y):\n",
        "    self.root = self._build_tree(X, y)\n",
        "\n",
        "  def _build_tree(self, X, y):\n",
        "    n_samples, n_features = X.shape\n",
        "    n_labels = len(np.unique(y))\n",
        "\n",
        "    if(n_labels == 1):\n",
        "      leaf_value = self._most_common_label(y)\n",
        "      return Node(value=leaf_value)\n",
        "\n",
        "    best_feature, best_threshold = self._best_split(X, y)\n",
        "\n",
        "    left_idxs , right_idxs = self._split(X[:, best_feature], best_threshold)\n",
        "    left = self._build_tree(X[left_idxs, :], y[left_idxs])\n",
        "    right = self._build_tree(X[right_idxs, :], y[right_idxs])\n",
        "    return Node(best_feature, best_threshold, left, right)\n",
        "\n",
        "  def _most_common_label(self, y):\n",
        "    unique_labels, counts = np.unique(y, return_counts=True)\n",
        "    return unique_labels[np.argmax(counts)]\n",
        "\n",
        "  def _split(self, X_column, split_threshold):\n",
        "    left_idxs = np.argwhere(X_column <= split_threshold).flatten()\n",
        "    right_idxs = np.argwhere(X_column > split_threshold).flatten()\n",
        "    return left_idxs, right_idxs\n",
        "\n",
        "  def _best_split(self, X, y):\n",
        "    best_gain, split_idx, split_threshold = float(\"inf\"), None, None\n",
        "\n",
        "    for feat_idx in range(X.shape[1]):\n",
        "      X_Column = X[:, feat_idx]\n",
        "      X_Column_sorted = np.sort(X_Column)\n",
        "      thresholds = (X_Column_sorted[:-1] + X_Column_sorted[1:])/2\n",
        "\n",
        "      for threshold in thresholds:\n",
        "        gain = self._information_gain(y, X_Column, threshold)\n",
        "\n",
        "        if gain < best_gain:\n",
        "          best_gain = gain\n",
        "          split_idx = feat_idx\n",
        "          split_threshold = threshold\n",
        "\n",
        "    return split_idx, split_threshold\n",
        "\n",
        "  def _information_gain(self, y, X_column, threshold):\n",
        "    # parent_entropy = self._entropy(y)\n",
        "\n",
        "    left_idxs, right_idxs = self._split(X_column, threshold)\n",
        "\n",
        "    if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
        "      return 0\n",
        "\n",
        "    n, n_l, n_r = len(y), len(left_idxs), len(right_idxs)\n",
        "    e_l, e_r = self._entropy(y[left_idxs]), self._entropy(y[right_idxs])\n",
        "    child_entropy = (n_l/n)*e_l + (n_r/n)*e_r\n",
        "    # information_gain = parent_entropy - child_entropy\n",
        "    information_gain = child_entropy\n",
        "    return information_gain\n",
        "\n",
        "  def _entropy(self, y):\n",
        "    fid3 = np.mean(y)\n",
        "\n",
        "    if fid3 == 0 or fid3 == 1:\n",
        "      return 0\n",
        "    else:\n",
        "      return -fid3*np.log(fid3) - (1-fid3)*np.log(1-fid3)\n",
        "\n",
        "  def predict(self, X):\n",
        "    predictions = np.array([self._traverse_tree(x, self.root) for x in X])\n",
        "    return predictions\n",
        "\n",
        "  def _traverse_tree(self, x, node):\n",
        "    if node.is_leaf_node():\n",
        "      return node.value\n",
        "\n",
        "    if x[node.feature] <= node.threshold:\n",
        "      return self._traverse_tree(x, node.left)\n",
        "    return self._traverse_tree(x, node.right)"
      ],
      "metadata": {
        "id": "7QitzJxS14WF"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([\n",
        "    [3, 7],\n",
        "    [1, 8],\n",
        "    [4, 5],\n",
        "    [2, 6]\n",
        "])\n",
        "\n",
        "y = np.array([1, 0, 1, 0])\n",
        "\n",
        "clf = DecisionTree()\n",
        "clf.fit(X, y)\n",
        "predictions = clf.predict(X)\n",
        "\n",
        "def accuracy(y_test, y_pred):\n",
        "  return np.mean(y_test == y_pred)\n",
        "\n",
        "acc = accuracy(y, predictions)\n",
        "print(acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_ik75kd9QYw",
        "outputId": "aec39dd2-557f-495a-cc86-887f866a340b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n"
          ]
        }
      ]
    }
  ]
}